import os
import re
import sys

# --- LLM Fingerprinting (Highly Simplified for 6-Hour Hackathon) ---

# Define some highly simplified "LLM signatures" based on common patterns
# In a real system, this would involve complex stylometric models,
# embeddings, or even watermarking detection.
# For hackathon, we look for tell-tale phrases.
LLM_SIGNATURES = {
    "Likely AI (General)": {
        "phrases": [
            "as an AI language model",
            "I cannot fulfill this request",
            "I do not have personal opinions",
            "I am a large language model",
            "it is important to note that",
            "in conclusion, it is evident that"
        ],
        "description": "Text contains common phrasing characteristic of large language models.",
        "confidence_score": 0.8
    },
    "Likely Human-Written": {
        "phrases": [
            # Phrases less commonly generated by AI, or more nuanced human expressions
            # This is more challenging to define negatively, so focus on positive AI signs.
        ],
        "description": "Text does not strongly match known AI patterns (needs more sophisticated analysis to confirm human origin).",
        "confidence_score": 0.2 # Lower confidence for 'human' by absence of AI
    }
}

def identify_llm_origin_simplified(text_content: str):
    if not text_content or not text_content.strip():
        return {"llm_origin": "N/A", "confidence": 0, "reason": "No text provided for LLM analysis."}

    text_lower = text_content.lower()

    for llm_type, signature in LLM_SIGNATURES.items():
        if llm_type == "Likely Human-Written":
            continue # Handle human case at the end if no AI patterns are found

        for phrase in signature["phrases"]:
            if phrase in text_lower:
                return {
                    "llm_origin": llm_type,
                    "confidence": signature["confidence_score"],
                    "reason": f"{signature['description']} (Found phrase: '{phrase}')"
                }

    # If no strong AI signature is found
    return {
        "llm_origin": "Human/Uncertain",
        "confidence": LLM_SIGNATURES["Likely Human-Written"]["confidence_score"],
        "reason": LLM_SIGNATURES["Likely Human-Written"]["description"]
    }


# --- Placeholder for Deepfake Detection (Skipped for 6-Hour Sprint) ---
# This function is here to maintain structure but will return a placeholder result.
def detect_deepfake_anomaly_simplified(media_paths: dict):
    # For a 6-hour hackathon, we'll indicate this is not actively analyzed.
    # If you happen to have a *very* obvious synthetic image you want to use
    # and hardcode its detection, you could, but generally skip for speed.

    if media_paths.get("video") or media_paths.get("image"):
        return {"deepfake_detected": False, "reason": "Deepfake visual analysis (LS-ZLF) is conceptual for Round 1 demo."}
    else:
        return {"deepfake_detected": False, "reason": "No visual media provided for deepfake analysis."}


# --- Main LS-ZLF Analysis Function ---
async def analyze_ls_zlf(media_paths: dict):
    results = {
        "deepfake_analysis": detect_deepfake_anomaly_simplified(media_paths),
        "llm_origin_analysis": {}
    }

    # LLM origin analysis uses the text content
    if media_paths.get("text") and os.path.exists(media_paths["text"]):
        try:
            with open(media_paths["text"], 'r', encoding='utf-8') as f:
                text_content = f.read()
                results["llm_origin_analysis"] = identify_llm_origin_simplified(text_content)
        except Exception as e:
            print(f"Error reading text file for LS-ZLF: {e}", file=sys.stderr)
            results["llm_origin_analysis"] = {"llm_origin": "ERROR", "confidence": 0, "reason": f"Could not read text for analysis: {str(e)}"}
    else:
        results["llm_origin_analysis"] = {"llm_origin": "N/A", "confidence": 0, "reason": "No text provided for LLM analysis."}

    return results